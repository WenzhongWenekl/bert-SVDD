{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-10T09:24:19.038938Z","iopub.execute_input":"2022-08-10T09:24:19.039400Z","iopub.status.idle":"2022-08-10T09:24:19.056602Z","shell.execute_reply.started":"2022-08-10T09:24:19.039359Z","shell.execute_reply":"2022-08-10T09:24:19.055153Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/msl-lzq/MSL_train.npy\n/kaggle/input/msl-lzq/MSL_test.npy\n/kaggle/input/msl-lzq/MSL_test_label.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"# The following is from base file:\n\n\n\n##################################################################\n\nfrom abc import ABC, abstractmethod # abc是一个抽象子类，现在有点难理解， 问问别人 放两个链接在这儿 # https://docs.python.org/zh-cn/3/library/abc.html # https://blog.csdn.net/weixin_40907382/article/details/80277170\nfrom torch.utils.data import DataLoader\n\n\n\nimport torch\nimport os\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport numpy as np\nimport collections\nimport numbers\nimport math\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\n\n\n\n\n# def get_loader_segment(data_path, batch_size, win_size=100, step=100, mode='train', dataset='MSL'):\n#     dataset = MSLSegLoader(data_path, win_size, 1, mode)\n\n#     shuffle = False\n#     if mode == 'train':\n#         shuffle = True\n\n#     data_loader = DataLoader(dataset=dataset,\n#                              batch_size=batch_size,\n#                              shuffle=shuffle,\n#                              num_workers=0)\n#     return data_loader\n\n\n\nclass BaseADDataset(ABC):\n    \"\"\"Anomaly detection dataset base class.\"\"\"\n    \n    \"\"\"为了加载数据的一个类，需要有区分 看看后面怎么改\"\"\"\n    \n    def __init__(self, root: str):\n        super().__init__()\n        self.root = root  # root path to data\n        \n  # 代码这里选择了两种类\n\n        self.n_classes = 2  # 0: normal, 1: outlier\n        self.normal_classes = None  # tuple with original class labels that define the normal class\n        self.outlier_classes = None  # tuple with original class labels that define the outlier class\n\n        self.train_set = None  # must be of type torch.utils.data.Dataset ！！！！！！！一定要是这个类型的数据\n        self.test_set = None  # must be of type torch.utils.data.Dataset\n\n        \n    \n    @abstractmethod\n    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (DataLoader, DataLoader):\n        \"\"\"Implement data loaders of type torch.utils.data.DataLoader for train_set and test_set.\"\"\"\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n    \n    \n    \n##################################################################\nimport logging  # 日志模块\n# logging模块定义的函数和类为应用程序和库的开发实现了一个灵活的事件日志系统。\n# logging模块是Python的一个标准库模块，由标准库模块提供日志记录API的关键好处是所有Python模块都可以使用这个日志记录功能。\n# 所以，你的应用日志可以将你自己的日志信息与来自第三方模块的信息整合起来。\nimport torch.nn as nn\nimport numpy as np\n\n\nclass BaseNet(nn.Module):\n    \"\"\"Base class for all neural networks.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__) # 得到一个Logger对象\n        self.rep_dim = None  # representation dimensionality, i.e. dim of the last layer\n\n    def forward(self, *input):\n        \"\"\"\n        Forward pass logic\n        :return: Network output\n        \"\"\"\n        raise NotImplementedError\n\n    def summary(self):\n        \"\"\"Network summary.\"\"\"\n        net_parameters = filter(lambda p: p.requires_grad, self.parameters())\n        params = sum([np.prod(p.size()) for p in net_parameters])\n        \n        \"\"\"logger.info 创建一个与它们的方法名对应等级的日志记录\"\"\"\n        self.logger.info('Trainable parameters: {}'.format(params))\n        self.logger.info(self)\n      \n    \n    \n##################################################################\nfrom abc import ABC, abstractmethod\n\"\"\"BaseTrainer 创建一个最基本的训练器，为后面提供便捷\"\"\"\nclass BaseTrainer(ABC):\n    \"\"\"Trainer base class.\"\"\"\n\n    def __init__(self, optimizer_name: str, lr: float, n_epochs: int, lr_milestones: tuple, batch_size: int,\n                 weight_decay: float, device: str, n_jobs_dataloader: int):\n        super().__init__()\n        self.optimizer_name = optimizer_name # 挑一个最喜欢的强化器\n        self.lr = lr # learning rate\n        self.n_epochs = n_epochs # 几层训练\n        self.lr_milestones = lr_milestones # 没懂\n        self.batch_size = batch_size \n        self.weight_decay = weight_decay # SVDD 的权重矩阵，但是不知道用在哪儿？？？\n        self.device = device # 到时候用gpu\n        self.n_jobs_dataloader = n_jobs_dataloader # 没懂\n        \n        \n    \"\"\" abstractmethod 相当于一个接口\n    \n        子类必须全部实现重写父类的abstractmethod方法\n        \n        非abstractmethod方法可以不实现重写\n        \n        带abstractmethod方法的类不能实例化\n        \n        链接 https://blog.csdn.net/u013210620/article/details/78604077\n    \"\"\"\n\n\n    @abstractmethod\n    def train(self, dataset: BaseADDataset, net: BaseNet) -> BaseNet:\n        \"\"\"\n        Implement train method that trains the given network using the train_set of dataset.\n        :return: Trained net\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def test(self, dataset: BaseADDataset, net: BaseNet):\n        \"\"\"\n        Implement test method that evaluates the test_set of dataset on the given network.\n        \"\"\"\n        pass\n\n    \n    \n##################################################################\nfrom torch.utils.data import DataLoader\n\"\"\" DataLoader：\n    就是数据加载器，结合了数据集和取样器，并且可以提供多个线程处理数据集。\n    \n    在训练模型时使用到此函数，用来把训练数据分成多个小组，此函数每次抛出一组数据。\n    \n    直至把所有的数据都抛出。就是做一个数据的初始化。\n\"\"\"\n\nclass TorchvisionDataset(BaseADDataset):\n    \"\"\"TorchvisionDataset class for datasets already implemented in torchvision.datasets.\"\"\"\n\n    def __init__(self, root: str):\n        super().__init__(root)\n    \n#     def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (DataLoader, DataLoader):\n#         train_loader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=shuffle_train,\n#                                   num_workers=num_workers)\n#         test_loader = DataLoader(dataset=self.test_set, batch_size=batch_size, shuffle=shuffle_test,\n#                                  num_workers=num_workers)\n#         return train_loader, test_loader\n    \n    \n    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (DataLoader, DataLoader):\n        \n\n#         shuffle = False\n#         if mode == 'train':\n        shuffle = True\n#         print(self.train_set)\n        data_loader = DataLoader(dataset=self.train_set,\n                                 batch_size=batch_size,\n                                 shuffle=shuffle_train,\n                                 num_workers=0)\n        return data_loader\n\nclass CIFAR10_Dataset(TorchvisionDataset):\n\n    def __init__(self, root: str, normal_class=5):\n        super().__init__(root)\n\n        self.n_classes = 2  # 0: normal, 1: outlier\n        self.normal_classes = tuple([normal_class])\n        self.outlier_classes = list(range(0, 10))\n\n        \n        self.train_set = MSLSegLoader(data_path = '../input/msl-lzq', win_size = 100, step = 1, mode = 'train')\n#         self.train_set = self.train_set.unsqueeze(0)\n        self.test_set = MSLSegLoader(data_path = '../input/msl-lzq', win_size = 100, step = 1, mode = 'test')\n#         self.test_set = self.test_set.unsqueeze(0)\n    \nclass MSLSegLoader(object):\n    def __init__(self, data_path, win_size, step, mode=\"train\"):\n        self.mode = mode\n        self.step = step\n        self.win_size = win_size\n        self.scaler = StandardScaler()\n        data = np.load(data_path + \"/MSL_test.npy\")\n        self.scaler.fit(data)\n        data = self.scaler.transform(data)\n        test_data = np.load(data_path + \"/MSL_test.npy\")\n        self.test = self.scaler.transform(test_data)\n\n        self.train = data\n        \n        self.val = self.test\n        self.test_labels = np.load(data_path + \"/MSL_test_label.npy\")\n#         print(\"test:\", self.test.shape)\n#         print(\"train:\", self.train.shape)\n\n    def __len__(self):\n\n        if self.mode == \"train\":\n            return (self.train.shape[0] - self.win_size) // self.step + 1\n        elif (self.mode == 'val'):\n            return (self.val.shape[0] - self.win_size) // self.step + 1\n        elif (self.mode == 'test'):\n            return (self.test.shape[0] - self.win_size) // self.step + 1\n        else:\n            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n\n    def __getitem__(self, index):\n        index = index * self.step\n        if self.mode == \"train\":\n#             print('aaaaaaa', type(np.float32(self.train[index:index + self.win_size])))\n#             print('bbbbbbb', type( np.float32(self.test_labels[0:self.win_size])))\n#             print('aaaaaaa', np.float32(self.train[index:index + self.win_size]))\n#             print('bbbbbbb',  np.float32(self.test_labels[0:self.win_size]))\n            Train = np.float32(self.train[index:index + self.win_size])\n#             print(Train.shape)\n#             Train = np.reshape(Train,(1,100,55))\n            return Train, np.float32(self.test_labels[0:self.win_size])\n        elif (self.mode == 'val'):\n            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n        elif (self.mode == 'test'):\n            Test = np.float32(self.test[index:index + self.win_size])\n#             Test = np.reshape(Test,(1,100,55))\n            return Test, np.float32(self.test_labels[index:index + self.win_size])\n        else:\n            return np.float32(self.test[\n                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:24:19.159346Z","iopub.execute_input":"2022-08-10T09:24:19.159952Z","iopub.status.idle":"2022-08-10T09:24:19.258635Z","shell.execute_reply.started":"2022-08-10T09:24:19.159892Z","shell.execute_reply":"2022-08-10T09:24:19.255315Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# The following is from optim file:\n\n\n##################################################################\nfrom sklearn.metrics import roc_auc_score\nimport logging\nimport time\nimport torch\nimport torch.optim as optim\nimport numpy as np\n\n\n# 从这里调用了 BaseTrainer\nclass AETrainer(BaseTrainer):\n\n    \"\"\"\n    \n    所有的设置都在这里改！！！！！！！！\n    我先降低了“n_epochs“ = 5\n    \"\"\"\n    \n    def __init__(self, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 5, lr_milestones: tuple = (),\n                 batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda', n_jobs_dataloader: int = 0):\n        \n        \n        super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device, n_jobs_dataloader)\n\n    def train(self, dataset: BaseADDataset, ae_net: BaseNet):\n        logger = logging.getLogger()\n\n        # Set device for network \n        \"\"\" 调用GPU \"\"\"\n        ae_net = ae_net.to(self.device)\n\n        \n        # Get train data loader\n        \"\"\" 读取训练集数据 \"\"\"\n        train_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n\n        \n        # Set optimizer (Adam optimizer for now)\n        \"\"\" 设置优化器！！！！！！！！！ \"\"\"\n        optimizer = optim.Adam(ae_net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n                               amsgrad=self.optimizer_name == 'amsgrad')\n\n        \n        # Set learning rate scheduler\n        \"\"\" 设置学习率。。。 \"\"\"\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n\n        \n        # Training\n        logger.info('Starting pretraining...')\n        start_time = time.time()\n        ae_net.train()\n        for epoch in range(self.n_epochs):\n\n            scheduler.step()\n            if epoch in self.lr_milestones:\n                logger.info('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n\n            loss_epoch = 0.0\n            n_batches = 0\n            epoch_start_time = time.time()\n            for data in train_loader:\n                try:\n                    inputs, _, _ = data\n                except:\n                    inputs, _ = data\n                \"\"\"\"\"\"\n                    \n#                 print('input shape:', inputs.shape)\n#                 print('input type:', type(inputs))\n                \n                \n                \"\"\"\"\"\"\n                inputs = inputs.to(self.device)\n\n                # Zero the network parameter gradients\n                optimizer.zero_grad()\n\n                # Update network parameters via backpropagation: forward + backward + optimize\n                outputs = ae_net(inputs)\n#                 print('input shape:', inputs.shape)\n#                 print('output shape:', outputs.shape)\n                scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n                loss = torch.mean(scores)\n                loss.backward()\n                optimizer.step()\n\n                loss_epoch += loss.item()\n                n_batches += 1\n\n            # log epoch statistics\n            epoch_train_time = time.time() - epoch_start_time\n            logger.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n                        .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches))\n\n        pretrain_time = time.time() - start_time\n        logger.info('Pretraining time: %.3f' % pretrain_time)\n        logger.info('Finished pretraining.')\n\n        return ae_net\n\n    def test(self, dataset: BaseADDataset, ae_net: BaseNet):\n        logger = logging.getLogger()\n\n        # Set device for network\n        ae_net = ae_net.to(self.device)\n\n        # Get test data loader\n        test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n\n        # Testing\n        logger.info('Testing autoencoder...')\n        loss_epoch = 0.0\n        n_batches = 0\n        start_time = time.time()\n        idx_label_score = []\n        ae_net.eval()\n        with torch.no_grad():\n            for data in test_loader:\n                # 这里label后删除了idx！！！！！！\n                inputs, labels = data\n                inputs = inputs.to(self.device)\n                outputs = ae_net(inputs)\n                scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n                loss = torch.mean(scores)\n\n                # Save triple of (idx, label, score) in a ， 这里删除了 idx： idx.cpu().data.numpy().tolist(),\n                idx_label_score += list(zip(\n                                            labels.cpu().data.numpy().tolist(),\n                                            scores.cpu().data.numpy().tolist()))\n\n                loss_epoch += loss.item()\n                n_batches += 1\n\n        logger.info('Test set Loss: {:.8f}'.format(loss_epoch / n_batches))\n        # 这里删除了 idx： _, \n        labels, scores = zip(*idx_label_score)\n        labels = np.array(labels)\n        scores = np.array(scores)\n        print(\"lables: \", labels)\n        print(\"scores: \", scores)\n#         auc = roc_auc_score(labels, scores)\n#         logger.info('Test set AUC: {:.2f}%'.format(100. * auc))\n\n        test_time = time.time() - start_time\n        logger.info('Autoencoder testing time: %.3f' % test_time)\n        logger.info('Finished testing autoencoder.')\n\n        \n        \n        \n        \n        \n##############################################################################\nfrom torch.utils.data.dataloader import DataLoader\nfrom sklearn.metrics import roc_auc_score\n\nimport logging\nimport time\nimport torch\nimport torch.optim as optim\nimport numpy as np\n\n\"\"\" n_epochs 被调成了5！！！！ \"\"\"\n\n\nclass DeepSVDDTrainer(BaseTrainer):\n\n    def __init__(self, objective, R, c, nu: float, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 5,\n                 lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n                 n_jobs_dataloader: int = 0):\n        super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device,\n                         n_jobs_dataloader)\n        \n        \n        \"\"\" \n        选择是单分类还是 软边界，应该是基于train集是否有标签？？？\n        \n        \"\"\"\n\n        assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n        self.objective = objective\n\n        \"\"\" \n        \n        Deep SVDD parameters\n        \n        设置Deep SVDD的半径，圆心和 “nu”---不知道是啥 ？？？\n        \n        \"\"\"\n        # Deep SVDD parameters\n        self.R = torch.tensor(R, device=self.device)  # radius R initialized with 0 by default.\n        self.c = torch.tensor(c, device=self.device) if c is not None else None\n        self.nu = nu\n\n        \n        \"\"\" \n        \n        Optimization parameters\n        \n        在半径更新前，跑几个epochs\n        \n        \"\"\"\n#         self.warm_up_n_epochs = 10  # number of training epochs for soft-boundary Deep SVDD before radius R gets updated\n        self.warm_up_n_epochs = 1 # for test\n        # Results\n        self.train_time = None\n        self.test_auc = None\n        self.test_time = None\n        self.test_scores = None\n\n        \n    \"\"\"\n    \n    最终的train函数\n    \n    参考上文标注\n    \n    \"\"\"\n    \n    def train(self, dataset: BaseADDataset, net: BaseNet):\n        logger = logging.getLogger()\n\n        # Set device for network\n        net = net.to(self.device)\n\n        # Get train data loader\n        train_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n\n        # Set optimizer (Adam optimizer for now)\n        optimizer = optim.Adam(net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n                               amsgrad=self.optimizer_name == 'amsgrad')\n\n        # Set learning rate scheduler\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n\n        # Initialize hypersphere center c (if c not loaded)\n        if self.c is None:\n            logger.info('Initializing center c...')\n            self.c = self.init_center_c(train_loader, net)\n            logger.info('Center c initialized.')\n\n        # Training\n        logger.info('Starting training...')\n        start_time = time.time()\n        net.train()\n        for epoch in range(self.n_epochs):\n\n            scheduler.step()\n            if epoch in self.lr_milestones:\n                logger.info('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n\n            loss_epoch = 0.0\n            n_batches = 0\n            epoch_start_time = time.time()\n            for data in train_loader:\n                try:\n                    inputs, _, _ = data\n                except:\n                    inputs, _ = data\n#                 print(inputs)\n                inputs = inputs.to(self.device)\n\n                # Zero the network parameter gradients\n                optimizer.zero_grad()\n\n                # Update network parameters via backpropagation: forward + backward + optimize\n                outputs = net(inputs)\n                dist = torch.sum((outputs - self.c) ** 2, dim=1)\n                if self.objective == 'soft-boundary':\n                    scores = dist - self.R ** 2\n                    loss = self.R ** 2 + (1 / self.nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n                else:\n                    loss = torch.mean(dist)\n                loss.backward()\n                optimizer.step()\n\n                # Update hypersphere radius R on mini-batch distances\n                if (self.objective == 'soft-boundary') and (epoch >= self.warm_up_n_epochs):\n                    self.R.data = torch.tensor(get_radius(dist, self.nu), device=self.device)\n\n                loss_epoch += loss.item()\n                n_batches += 1\n\n            # log epoch statistics 记录一下训练过程\n            epoch_train_time = time.time() - epoch_start_time\n            logger.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n                        .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches))\n\n        self.train_time = time.time() - start_time\n        logger.info('Training time: %.3f' % self.train_time)\n\n        logger.info('Finished training.')\n\n        return net\n\n    def test(self, dataset: BaseADDataset, net: BaseNet):\n        logger = logging.getLogger()\n\n        # Set device for network\n        net = net.to(self.device)\n\n        # Get test data loader\n        test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n\n        # Testing\n        logger.info('Starting testing...')\n        start_time = time.time()\n        idx_label_score = []\n        net.eval()\n        with torch.no_grad():\n            for data in test_loader:\n                # 这里label后删除了idx！！！！！！\n                inputs, labels = data\n                inputs = inputs.to(self.device)\n                outputs = net(inputs)\n                dist = torch.sum((outputs - self.c) ** 2, dim=1)\n                if self.objective == 'soft-boundary':\n                    scores = dist - self.R ** 2\n                else:\n                    scores = dist\n\n                # Save triples of (idx, label, score) in a list, 这里把 idx 给删除了\n                \n                \n                idx_label_score += list(zip(\n                                            labels.cpu().data.numpy().tolist(),\n                                            scores.cpu().data.numpy().tolist()))\n\n        self.test_time = time.time() - start_time\n        logger.info('Testing time: %.3f' % self.test_time)\n\n        self.test_scores = idx_label_score\n\n        # Compute AUC 这里删除了 idx： _,\n        labels, scores = zip(*idx_label_score)\n        labels = np.array(labels)\n        scores = np.array(scores)\n        \n#         print(\"lables: \", labels)\n#         print(\"scores: \", scores)\n\n#         self.test_auc = roc_auc_score(labels, scores)\n#         logger.info('Test set AUC: {:.2f}%'.format(100. * self.test_auc))\n\n        logger.info('Finished testing.')\n\n        \n        \n    def init_center_c(self, train_loader: DataLoader, net: BaseNet, eps=0.1):\n        \n        \"\"\"\n        \n        用均值初始化圆心\n        \n        Initialize hypersphere center c as the mean from an initial forward pass on the data.\n        \n        \"\"\"\n        \n        n_samples = 0\n        \n        ''' 修改圆心为2维'''\n        \n#         c = torch.zeros(net.rep_dim, device=self.device)\n        c = torch.zeros([100,3], device=self.device)\n\n        net.eval()\n        with torch.no_grad():\n            for data in train_loader:\n                # get the inputs of the batch\n                try:\n                    inputs, _, _ = data\n                except:\n                    inputs, _ = data\n                inputs = inputs.to(self.device)\n                outputs = net(inputs)\n#                 print('output: ', outputs)\n#                 print('output shape: ', outputs.shape)\n\n                n_samples += outputs.shape[0]\n                c += torch.sum(outputs, dim=0)\n\n        c /= n_samples\n\n        # If c_i is too close to 0, set to +-eps. Reason: a zero unit can be trivially matched with zero weights.\n        c[(abs(c) < eps) & (c < 0)] = -eps\n        c[(abs(c) < eps) & (c > 0)] = eps\n\n        return c\n\n\n    \ndef get_radius(dist: torch.Tensor, nu: float):\n    \n    \"\"\"\n    \n    计算半径\n    \n    Optimally solve for radius R via the (1-nu)-quantile of distances.\n    \n    \"\"\"\n    return np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-08-10T09:24:19.277703Z","iopub.execute_input":"2022-08-10T09:24:19.280608Z","iopub.status.idle":"2022-08-10T09:24:19.379634Z","shell.execute_reply.started":"2022-08-10T09:24:19.280551Z","shell.execute_reply":"2022-08-10T09:24:19.377984Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# the full functions of Deep_SVDD\n\nimport json\nimport torch\n\nclass DeepSVDD(object):\n    \"\"\"A class for the Deep SVDD method.\n\n    Attributes:\n        objective: A string specifying the Deep SVDD objective (either 'one-class' or 'soft-boundary').\n        nu: Deep SVDD hyperparameter nu (must be 0 < nu <= 1).\n        R: Hypersphere radius R.\n        c: Hypersphere center c.\n        net_name: A string indicating the name of the neural network to use.\n        net: The neural network \\phi.\n        ae_net: The autoencoder network corresponding to \\phi for network weights pretraining.\n        trainer: DeepSVDDTrainer to train a Deep SVDD model.\n        optimizer_name: A string indicating the optimizer to use for training the Deep SVDD network.\n        ae_trainer: AETrainer to train an autoencoder in pretraining.\n        ae_optimizer_name: A string indicating the optimizer to use for pretraining the autoencoder.\n        results: A dictionary to save the results.\n    \"\"\"\n\n    def __init__(self, objective: str = 'one-class', nu: float = 0.1):\n        \"\"\"Inits DeepSVDD with one of the two objectives and hyperparameter nu.\"\"\"\n\n        assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n        self.objective = objective\n        assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n        self.nu = nu\n        self.R = 0.0  # hypersphere radius R\n        self.c = None  # hypersphere center c\n\n        self.net_name = None\n        self.net = None  # neural network \\phi\n\n        self.trainer = None\n        self.optimizer_name = None\n\n        self.ae_net = None  # autoencoder network for pretraining\n        self.ae_trainer = None\n        self.ae_optimizer_name = None\n\n        self.results = {\n            'train_time': None,\n            'test_auc': None,\n            'test_time': None,\n            'test_scores': None,\n        }\n\n    def set_network(self, net_name):\n        \"\"\"Builds the neural network \\phi.\"\"\"\n        self.net_name = net_name\n        self.net = build_network(net_name)\n\n        \n        \"\"\" n_epochs = 50 \"\"\"\n        \n    def train(self, dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 5,\n              lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n              n_jobs_dataloader: int = 0):\n        \"\"\"Trains the Deep SVDD model on the training data.\"\"\"\n\n        self.optimizer_name = optimizer_name\n        self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu, optimizer_name, lr=lr,\n                                       n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n                                       weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader)\n        # Get the model\n        self.net = self.trainer.train(dataset, self.net)\n        self.R = float(self.trainer.R.cpu().data.numpy())  # get float\n        self.c = self.trainer.c.cpu().data.numpy().tolist()  # get list\n        self.results['train_time'] = self.trainer.train_time\n\n    def test(self, dataset: BaseADDataset, device: str = 'cuda', n_jobs_dataloader: int = 0):\n        \"\"\"Tests the Deep SVDD model on the test data.\"\"\"\n\n        if self.trainer is None:\n            self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu,\n                                           device=device, n_jobs_dataloader=n_jobs_dataloader)\n\n        self.trainer.test(dataset, self.net)\n        # Get results\n        self.results['test_auc'] = self.trainer.test_auc\n        self.results['test_time'] = self.trainer.test_time\n        self.results['test_scores'] = self.trainer.test_scores\n\n        \n    \"\"\"\n    \n    提前为Deep SVDD 训练一个 权重\n    n_epochs: int = 100\n    \n    \"\"\"\n    \n    def pretrain(self, dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 5,\n                 lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n                 n_jobs_dataloader: int = 0):\n        \"\"\"Pretrains the weights for the Deep SVDD network \\phi via autoencoder.\"\"\"\n\n        self.ae_net = build_autoencoder(self.net_name)\n#         print(self.ae_net.shape)\n        self.ae_optimizer_name = optimizer_name\n        self.ae_trainer = AETrainer(optimizer_name, lr=lr, n_epochs=n_epochs, lr_milestones=lr_milestones,\n                                    batch_size=batch_size, weight_decay=weight_decay, device=device,\n                                    n_jobs_dataloader=n_jobs_dataloader)\n        \n        \n        \"\"\"\"\"\"\n        self.ae_net = self.ae_trainer.train(dataset, self.ae_net)\n        \"\"\"\"\"\"\n        \n        \n        self.ae_trainer.test(dataset, self.ae_net)\n        self.init_network_weights_from_pretraining()\n\n        \n    def init_network_weights_from_pretraining(self):\n        \n        \"\"\"Initialize the Deep SVDD network weights from the encoder weights of the pretraining autoencoder.\"\"\"\n\n        net_dict = self.net.state_dict()\n        ae_net_dict = self.ae_net.state_dict()\n\n        # Filter out decoder network keys\n        ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n        # Overwrite values in the existing state_dict\n        net_dict.update(ae_net_dict)\n        # Load the new state_dict\n        self.net.load_state_dict(net_dict)\n\n        \n    def save_model(self, export_model, save_ae=True):\n        \n        \"\"\"Save Deep SVDD model to export_model.\"\"\"\n\n        net_dict = self.net.state_dict()\n        ae_net_dict = self.ae_net.state_dict() if save_ae else None\n\n        torch.save({'R': self.R,\n                    'c': self.c,\n                    'net_dict': net_dict,\n                    'ae_net_dict': ae_net_dict}, export_model)\n\n    def load_model(self, model_path, load_ae=False):\n        \n        \"\"\"Load Deep SVDD model from model_path.\"\"\"\n\n        model_dict = torch.load(model_path)\n\n        self.R = model_dict['R']\n        self.c = model_dict['c']\n        self.net.load_state_dict(model_dict['net_dict'])\n        if load_ae:\n            if self.ae_net is None:\n                self.ae_net = build_autoencoder(self.net_name)\n            self.ae_net.load_state_dict(model_dict['ae_net_dict'])\n\n            \n    def save_results(self, export_json):\n        \"\"\"Save results dict to a JSON-file.\"\"\"\n        with open(export_json, 'w') as fp:\n            json.dump(self.results, fp)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:24:19.387225Z","iopub.execute_input":"2022-08-10T09:24:19.391121Z","iopub.status.idle":"2022-08-10T09:24:19.438307Z","shell.execute_reply.started":"2022-08-10T09:24:19.391038Z","shell.execute_reply":"2022-08-10T09:24:19.436668Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# net work and autoencoder\nimport torch.nn.functional as F\n\nclass CIFAR10_LeNet_ELU(BaseNet):\n\n    def __init__(self):\n        super().__init__()\n\n        self.rep_dim = 3\n#         self.pool = nn.MaxPool2d(2, 2)\n\n#         self.conv1 = nn.Conv2d(1, 32, 5, bias=False, padding=2)\n#         self.bn2d1 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n#         self.conv2 = nn.Conv2d(32, 64, 5, bias=False, padding=2)\n#         self.bn2d2 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n#         self.conv3 = nn.Conv2d(64, 128, 5, bias=False, padding=2)\n#         self.bn2d3 = nn.BatchNorm2d(128, eps=1e-04, affine=False)\n#         self.fc1 = nn.Linear(128 * 4 * 4, self.rep_dim, bias=False)\n        self.encoder1 = nn.Sequential(\n            nn.Linear(55, 48),\n            nn.ReLU(True),\n            nn.Linear(48, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 12),\n            nn.ReLU(True), \n            nn.Linear(12, 3))\n        \n#         self.linear1 = nn.Linear(100 * 55, 10 * 5)\n\n        \n\n    def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.pool(F.elu(self.bn2d1(x)))\n#         x = self.conv2(x)\n#         x = self.pool(F.elu(self.bn2d2(x)))\n#         x = self.conv3(x)\n#         x = self.pool(F.elu(self.bn2d3(x)))\n#         x = x.view(x.size(0), -1)\n#         x = self.fc1(x)\n\n\n        x = self.encoder1(x)\n        return x\n\n\nclass CIFAR10_LeNet_ELU_Autoencoder(BaseNet):\n\n    def __init__(self):\n        super().__init__()\n\n        self.rep_dim = 3\n#         self.pool = nn.MaxPool2d(2,2)\n#         # Encoder (must match the Deep SVDD network above)\n#         self.conv1 = nn.Conv2d(1, 32, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.conv1.weight)\n#         self.bn2d1 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n#         self.conv2 = nn.Conv2d(32, 64, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.conv2.weight)\n#         self.bn2d2 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n#         self.conv3 = nn.Conv2d(64, 128, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.conv3.weight)\n#         self.bn2d3 = nn.BatchNorm2d(128, eps=1e-04, affine=False)\n#         self.fc1 = nn.Linear(9216, self.rep_dim, bias=False)\n#         self.bn1d = nn.BatchNorm1d(self.rep_dim, eps=1e-04, affine=False)\n\n\n\n#         # Decoder\n#         self.deconv1 = nn.ConvTranspose2d(int(self.rep_dim / (4 * 4)), 128, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.deconv1.weight)\n#         self.bn2d4 = nn.BatchNorm2d(128, eps=1e-04, affine=False)\n#         self.deconv2 = nn.ConvTranspose2d(128, 64, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.deconv2.weight)\n#         self.bn2d5 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n#         self.deconv3 = nn.ConvTranspose2d(64, 32, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.deconv3.weight)\n#         self.bn2d6 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n#         self.deconv4 = nn.ConvTranspose2d(32, 1, 5, bias=False, padding=2)\n#         nn.init.xavier_uniform_(self.deconv4.weight)\n        \n    \n        self.encoder = nn.Sequential(\n            nn.Linear(55, 48),\n            nn.ReLU(True),\n            nn.Linear(48, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 12),\n            nn.ReLU(True), \n            nn.Linear(12, 3))\n        \n        \n        self.decoder = nn.Sequential(\n            nn.Linear(3, 12),\n            nn.ReLU(True),\n            nn.Linear(12, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 48),\n            nn.ReLU(True),\n            nn.Linear(48, 55), \n            nn.Tanh())\n\n    def forward(self, x):\n#         x = self.conv1(x)\n#         print('1.shape: ', x.shape)\n#         x = self.pool(F.elu(self.bn2d1(x)))\n#         print('2.shape: ', x.shape)\n#         x = self.conv2(x)\n#         x = self.pool(F.elu(self.bn2d2(x)))\n#         x = self.conv3(x)\n#         x = self.pool(F.elu(self.bn2d3(x)))\n#         x = x.view(x.size(0), -1) # 9216\n#         x = self.bn1d(self.fc1(x))\n#         x = x.view(x.size(0), int(self.rep_dim / (4 * 4)), 4, 4)\n#         x = F.elu(x)\n#         x = self.deconv1(x)\n#         x = F.interpolate(F.elu(self.bn2d4(x)), scale_factor=2)\n#         x = self.deconv2(x)\n#         x = F.interpolate(F.elu(self.bn2d5(x)), scale_factor=2)\n#         x = self.deconv3(x)\n#         x = F.interpolate(F.elu(self.bn2d6(x)), scale_factor=2)\n#         x = self.deconv4(x)\n\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:24:19.446764Z","iopub.execute_input":"2022-08-10T09:24:19.450261Z","iopub.status.idle":"2022-08-10T09:24:19.474683Z","shell.execute_reply.started":"2022-08-10T09:24:19.450206Z","shell.execute_reply":"2022-08-10T09:24:19.472451Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def load_dataset():\n    \"\"\"Loads the dataset.\n    读取数据集， 要读取成\"\"\"\n    \n#     data  =  MSLSegLoader(data_path = '../input/msl-lzq', win_size = 100, step = 1, mode = 'train')\n    data = CIFAR10_Dataset(root = '../input/msl-lzq')\n    \n    return data\n# load_dataset\n\n\n\ndef build_network(net_name):\n    \"\"\"Builds the neural network.\"\"\"\n\n    net = CIFAR10_LeNet_ELU()\n\n    return net\n\n\ndef build_autoencoder(net_name):\n    \"\"\"Builds the corresponding autoencoder network.\"\"\"\n\n    ae_net = CIFAR10_LeNet_ELU_Autoencoder()\n\n    return ae_net\n\n\n\nclass Config(object):\n    \"\"\"Base class for experimental setting/configuration.\"\"\"\n\n    def __init__(self, settings):\n        self.settings = settings\n\n    def load_config(self, import_json):\n        \"\"\"Load settings dict from import_json (path/filename.json) JSON-file.\"\"\"\n\n        with open(import_json, 'r') as fp:\n            settings = json.load(fp)\n\n        for key, value in settings.items():\n            self.settings[key] = value\n\n    def save_config(self, export_json):\n        \"\"\"Save settings dict to export_json (path/filename.json) JSON-file.\"\"\"\n\n        with open(export_json, 'w') as fp:\n            json.dump(self.settings, fp)\n\n\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:24:19.481833Z","iopub.execute_input":"2022-08-10T09:24:19.482739Z","iopub.status.idle":"2022-08-10T09:24:19.501798Z","shell.execute_reply.started":"2022-08-10T09:24:19.482689Z","shell.execute_reply":"2022-08-10T09:24:19.499899Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#  the following is the main function\n\n#  记得： 记录一下每个参数的来源 ！！！！\n\nimport torch\nimport logging\nimport random\nimport numpy as np\n\n\ndef main(dataset_name, net_name, xp_path, data_path, load_config, load_model, objective, nu, device, seed, optimizer_name, lr, n_epochs, lr_milestone, batch_size, weight_decay, pretrain, ae_optimizer_name, ae_lr, ae_n_epochs, ae_lr_milestone, ae_batch_size, ae_weight_decay, n_jobs_dataloader, normal_class):\n    \n    \"\"\"\n    Deep SVDD, a fully deep method for anomaly detection.\n\n    :arg DATASET_NAME: Name of the dataset to load.\n    :arg NET_NAME: Name of the neural network to use.\n    :arg XP_PATH: Export path for logging the experiment.\n    :arg DATA_PATH: Root path of data.\n    \n    \"\"\"\n\n    # Get configuration\n    cfg = Config(locals().copy())\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    log_file = xp_path + '/log.txt'\n#     file_handler = logging.FileHandler(log_file)\n#     file_handler.setLevel(logging.INFO)\n#     file_handler.setFormatter(formatter)\n#     logger.addHandler(file_handler)\n\n#     # Print arguments\n#     logger.info('Log file is %s.' % log_file)\n#     logger.info('Data path is %s.' % data_path)\n#     logger.info('Export path is %s.' % xp_path)\n\n#     logger.info('Dataset: %s' % dataset_name)\n#     logger.info('Normal class: %d' % normal_class)\n#     logger.info('Network: %s' % net_name)\n\n    # If specified, load experiment config from JSON-file\n#     if load_config:\n#         cfg.load_config(import_json=load_config)\n#         logger.info('Loaded configuration from %s.' % load_config)\n\n#     # Print configuration\n#     logger.info('Deep SVDD objective: %s' % cfg.settings['objective'])\n#     logger.info('Nu-paramerter: %.2f' % cfg.settings['nu'])\n\n    # Set seed\n    if cfg.settings['seed'] != -1:\n        random.seed(cfg.settings['seed'])\n        np.random.seed(cfg.settings['seed'])\n        torch.manual_seed(cfg.settings['seed'])\n        logger.info('Set seed to %d.' % cfg.settings['seed'])\n\n    # Default device to 'cpu' if cuda is not available\n    if not torch.cuda.is_available():\n        device = 'cpu'\n    logger.info('Computation device: %s' % device)\n    logger.info('Number of dataloader workers: %d' % n_jobs_dataloader)\n\n    # Load data\n    dataset = load_dataset()\n#     dataset = MSLSegLoader(data_path = '../input/msl-lzq', win_size = 100, step = 1, mode = 'train')\n    # Initialize DeepSVDD model and set neural network \\phi\n    deep_SVDD = DeepSVDD(cfg.settings['objective'], cfg.settings['nu'])\n    deep_SVDD.set_network(net_name)\n    \n    # If specified, load Deep SVDD model (radius R, center c, network weights, and possibly autoencoder weights)\n    if load_model:\n        deep_SVDD.load_model(model_path=load_model, load_ae=True)\n        logger.info('Loading model from %s.' % load_model)\n\n    logger.info('Pretraining: %s' % pretrain)\n    if pretrain:\n        # Log pretraining details\n        logger.info('Pretraining optimizer: %s' % cfg.settings['ae_optimizer_name'])\n        logger.info('Pretraining learning rate: %g' % cfg.settings['ae_lr'])\n        logger.info('Pretraining epochs: %d' % cfg.settings['ae_n_epochs'])\n        logger.info('Pretraining learning rate scheduler milestones: %s' % (cfg.settings['ae_lr_milestone'],))\n        logger.info('Pretraining batch size: %d' % cfg.settings['ae_batch_size'])\n        logger.info('Pretraining weight decay: %g' % cfg.settings['ae_weight_decay'])\n\n        # Pretrain model on dataset (via autoencoder)\n        deep_SVDD.pretrain(dataset,\n                           optimizer_name=cfg.settings['ae_optimizer_name'],\n                           lr=cfg.settings['ae_lr'],\n                           n_epochs=cfg.settings['ae_n_epochs'],\n                           lr_milestones=cfg.settings['ae_lr_milestone'],\n                           batch_size=cfg.settings['ae_batch_size'],\n                           weight_decay=cfg.settings['ae_weight_decay'],\n                           device=device,\n                           n_jobs_dataloader=n_jobs_dataloader)\n\n    # Log training details\n    logger.info('Training optimizer: %s' % cfg.settings['optimizer_name'])\n    logger.info('Training learning rate: %g' % cfg.settings['lr'])\n    logger.info('Training epochs: %d' % cfg.settings['n_epochs'])\n    logger.info('Training learning rate scheduler milestones: %s' % (cfg.settings['lr_milestone'],))\n    logger.info('Training batch size: %d' % cfg.settings['batch_size'])\n    logger.info('Training weight decay: %g' % cfg.settings['weight_decay'])\n\n    # Train model on dataset\n    deep_SVDD.train(dataset,\n                    optimizer_name=cfg.settings['optimizer_name'],\n                    lr=cfg.settings['lr'],\n                    n_epochs=cfg.settings['n_epochs'],\n                    lr_milestones=cfg.settings['lr_milestone'],\n                    batch_size=cfg.settings['batch_size'],\n                    weight_decay=cfg.settings['weight_decay'],\n                    device=device,\n                    n_jobs_dataloader=n_jobs_dataloader)\n\n    # Test model\n    deep_SVDD.test(dataset, device=device, n_jobs_dataloader=n_jobs_dataloader)\n\n    # Plot most anomalous and most normal (within-class) test samples\n    '''这里把indices 给删了， 为不知道是干啥用的'''\n#     indices, labels, scores = zip(*deep_SVDD.results['test_scores'])\n    labels, scores = zip(*deep_SVDD.results['test_scores'])\n#     indices, labels, scores = np.array(indices), np.array(labels), np.array(scores)\n    scores = np.array(labels), np.array(scores)\n#     idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n\n#     if dataset_name in ('mnist', 'cifar10'):\n\n#         if dataset_name == 'mnist':\n#             X_normals = dataset.test_set.test_data[idx_sorted[:32], ...].unsqueeze(1)\n#             X_outliers = dataset.test_set.test_data[idx_sorted[-32:], ...].unsqueeze(1)\n\n#         if dataset_name == 'cifar10':\n#             X_normals = torch.tensor(np.transpose(dataset.test_set.test_data[idx_sorted[:32], ...], (0, 3, 1, 2)))\n#             X_outliers = torch.tensor(np.transpose(dataset.test_set.test_data[idx_sorted[-32:], ...], (0, 3, 1, 2)))\n\n    # Save results, model, and configuration\n    deep_SVDD.save_results(export_json=xp_path + '/results.json')\n    deep_SVDD.save_model(export_model=xp_path + '/model.tar')\n    cfg.save_config(export_json=xp_path + '/config.json')\n\n\nif __name__ == '__main__':\n    main(dataset_name = \"cifar10\", \n         net_name = 'Prof.Fan', \n         xp_path = '.', \n         data_path = '../input/msl-lzq/MSL_test.npy', \n         load_config = False, \n         load_model = False, \n         objective = 'soft-boundary', \n         nu = 0.1, \n         device = 'cuda', \n         seed = 1, \n         optimizer_name = 'adam', \n         lr = 0.001, \n         n_epochs = 5, \n         lr_milestone = (), \n         batch_size = 128, \n         weight_decay = 1e-6, \n         pretrain = True, \n         ae_optimizer_name = 'adam', \n         ae_lr = 0.001, \n         ae_n_epochs = 5, \n         ae_lr_milestone = (), \n         ae_batch_size = 128, \n         ae_weight_decay = 1e-6, \n         n_jobs_dataloader = 0, \n         normal_class = 0)\n\n    \nprint('done')","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:33:19.795172Z","iopub.execute_input":"2022-08-10T09:33:19.795664Z","iopub.status.idle":"2022-08-10T09:34:29.049425Z","shell.execute_reply.started":"2022-08-10T09:33:19.795625Z","shell.execute_reply":"2022-08-10T09:34:29.048239Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"lables:  [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nscores:  [1.65206592e+03 5.70078735e+02 6.24780273e+02 ... 4.35331494e-01\n 1.69170020e+03 9.79591553e+02]\ndone\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport torch\ndef read_data(address = \"../input/msl-lzq/MSL_test.npy\"):\n     return np.load(address)\n\nfile = read_data()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.757227Z","iopub.status.idle":"2022-08-10T09:25:16.758787Z","shell.execute_reply.started":"2022-08-10T09:25:16.758468Z","shell.execute_reply":"2022-08-10T09:25:16.758498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nprint(file.shape)\nint(np.prod(file.shape))\n# file = torch.from_numpy(file)\nfile[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.760898Z","iopub.status.idle":"2022-08-10T09:25:16.761936Z","shell.execute_reply.started":"2022-08-10T09:25:16.761607Z","shell.execute_reply":"2022-08-10T09:25:16.761639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from base.base_trainer import BaseTrainer\nfrom base.base_dataset import BaseADDataset\nfrom base.base_net import BaseNet\nfrom torch.utils.data.dataloader import DataLoader\nfrom sklearn.metrics import roc_auc_score\n\nimport logging\nimport time\nimport torch\nimport torch.optim as optim\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.763836Z","iopub.status.idle":"2022-08-10T09:25:16.764762Z","shell.execute_reply.started":"2022-08-10T09:25:16.764475Z","shell.execute_reply":"2022-08-10T09:25:16.764504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dsvdd import DeepSVDD\n","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.766557Z","iopub.status.idle":"2022-08-10T09:25:16.767423Z","shell.execute_reply.started":"2022-08-10T09:25:16.767105Z","shell.execute_reply":"2022-08-10T09:25:16.767129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef global_contrast_normalization(x, scale = 'l1'):\n    assert scale in ('l1', 'l2')\n    n_features = int(np.prod(x.shape)) # 所有维度特征数量和 n * w * h\n    mean = torch.mean(x)\n    x -= mean\n    if scale == 'l1':\n        x_scale = torch.mean(torch.abs(x)) \n    if scale == 'l2':\n        x_scale = torch.sqrt(torch.sum( x**2 )) / n_features \n    x /= x_scale\n    return x\n\ndata = global_contrast_normalization(file)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.769047Z","iopub.status.idle":"2022-08-10T09:25:16.769933Z","shell.execute_reply.started":"2022-08-10T09:25:16.769636Z","shell.execute_reply":"2022-08-10T09:25:16.769663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.771536Z","iopub.status.idle":"2022-08-10T09:25:16.772424Z","shell.execute_reply.started":"2022-08-10T09:25:16.772122Z","shell.execute_reply":"2022-08-10T09:25:16.772150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_center_c(self, train_loader, net, eps = 0.1):\n        n_samples = 0\n        c = torch.zeros(net.rep_dim, device = self.device)\n        net.eval()\n        with torch.no_grad():\n            for data in train_loader:\n                inputs, _ = data\n                inputs = inputs.to(self.device)\n                outputs = net(inputs)\n                n_samples += outputs.shape[0]\n                c += torch.sum(outputs, dim = 0)\n        c /= n_samples\n\n        c[(abs(c) < eps) & (c < 0)] = -eps \n        c[(abs(c) > eps) & (c > 0)] = eps\n        return c\n","metadata":{"execution":{"iopub.status.busy":"2022-08-10T09:25:16.774013Z","iopub.status.idle":"2022-08-10T09:25:16.774896Z","shell.execute_reply.started":"2022-08-10T09:25:16.774598Z","shell.execute_reply":"2022-08-10T09:25:16.774624Z"},"trusted":true},"execution_count":null,"outputs":[]}]}